{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec9a9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from keras.utils import to_categorical\n",
    "import pywt\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras import optimizers\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8393a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalaccuracy = []\n",
    "totalprecision = []\n",
    "totalrecall = []\n",
    "totalf1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7d650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "penghuni = os.listdir(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\Penghuni\")\n",
    "temen1 = os.listdir(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\Temen1\")\n",
    "bkn_penghuni = os.listdir(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\BukanPenghuni\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b89080e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "totalaccuracy = []\n",
    "totalprecision = []\n",
    "totalrecall = []\n",
    "totalf1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08dc3f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNbuild(height, width, classes, channels):\n",
    "    model = Sequential()\n",
    "    \n",
    "    inputShape = (height, width, channels,)\n",
    "    chanDim = -1\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        inputShape = (channels, height, width)\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = inputShape))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(BatchNormalization(axis = chanDim))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes, activation = 'softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6154672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in penghuni:\n",
    "\n",
    "    image = cv2.imread(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\Penghuni\\\\\"+i)\n",
    "    image_array = Image.fromarray(image , 'RGB')\n",
    "    resize_img = image_array.resize((50 , 50))\n",
    "    coeffs1 = pywt.dwt2(resize_img, 'bior1.3')\n",
    "    LL, (LH, HL, HH) = coeffs1\n",
    "    data.append(np.array(LL))\n",
    "    data.append(np.array(LH))\n",
    "    data.append(np.array(HL))\n",
    "    data.append(np.array(HH))\n",
    "    labels.append(0)\n",
    "    labels.append(0)\n",
    "    labels.append(0)\n",
    "    labels.append(0)\n",
    "\n",
    "for u in bkn_penghuni:\n",
    "    \n",
    "    image = cv2.imread(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\BukanPenghuni\\\\\"+u)\n",
    "    image_array = Image.fromarray(image , 'RGB')\n",
    "    resize_img = image_array.resize((50 , 50))\n",
    "    coeffs2 = pywt.dwt2(resize_img, 'bior1.3')\n",
    "    LL, (LH, HL, HH) = coeffs2\n",
    "    data.append(np.array(LL))\n",
    "    data.append(np.array(LH))\n",
    "    data.append(np.array(HL))\n",
    "    data.append(np.array(HH))\n",
    "    labels.append(1)\n",
    "    labels.append(1)\n",
    "    labels.append(1)\n",
    "    labels.append(1)\n",
    "\n",
    "for j in temen1:\n",
    "\n",
    "    image = cv2.imread(\"C:\\\\Users\\\\muham\\\\OneDrive\\\\Documents\\\\TA\\\\images\\\\train\\\\Temen1\\\\\"+j)\n",
    "    image_array = Image.fromarray(image , 'RGB')\n",
    "    resize_img = image_array.resize((50 , 50))\n",
    "    coeffs1 = pywt.dwt2(resize_img, 'bior1.3')\n",
    "    LL, (LH, HL, HH) = coeffs1\n",
    "    data.append(np.array(LL))\n",
    "    data.append(np.array(LH))\n",
    "    data.append(np.array(HL))\n",
    "    data.append(np.array(HH))\n",
    "    labels.append(2)\n",
    "    labels.append(2)\n",
    "    labels.append(2)\n",
    "    labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346b9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "np.save('Cells' , cells)\n",
    "np.save('Labels' , labels)\n",
    "\n",
    "n = np.arange(cells.shape[0])\n",
    "np.random.shuffle(n)\n",
    "cells = cells[n]\n",
    "labels = labels[n]\n",
    "\n",
    "cells = cells.astype(np.float32)\n",
    "labels = labels.astype(np.int32)\n",
    "cells = cells/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f607f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1, Train set: 1475, Test set:369\n",
      "====================\n",
      "Fold:  1\n",
      "====================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 25, 32)        1184      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 12, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 24, 12, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 12, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 22, 10, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 11, 5, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 11, 5, 32)         128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 11, 5, 32)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 3, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 4, 1, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 4, 1, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 1, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1539      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89699 (350.39 KB)\n",
      "Trainable params: 88483 (345.64 KB)\n",
      "Non-trainable params: 1216 (4.75 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/75\n",
      "37/37 [==============================] - 7s 73ms/step - loss: 1.7134 - accuracy: 0.3839 - val_loss: 0.9806 - val_accuracy: 0.5238\n",
      "Epoch 2/75\n",
      "37/37 [==============================] - 2s 62ms/step - loss: 1.3323 - accuracy: 0.4814 - val_loss: 0.9767 - val_accuracy: 0.4014\n",
      "Epoch 3/75\n",
      "37/37 [==============================] - 2s 56ms/step - loss: 1.1874 - accuracy: 0.5025 - val_loss: 0.9812 - val_accuracy: 0.4082\n",
      "Epoch 4/75\n",
      "37/37 [==============================] - 2s 51ms/step - loss: 1.1679 - accuracy: 0.5093 - val_loss: 1.0117 - val_accuracy: 0.3810\n",
      "Epoch 5/75\n",
      "18/37 [=============>................] - ETA: 0s - loss: 1.1424 - accuracy: 0.4896"
     ]
    }
   ],
   "source": [
    "kf =KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cnt = 1\n",
    "# split()  method generate indices to split data into training and test set.\n",
    "for train_index, test_index in kf.split(cells, labels):\n",
    "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
    "    print(\"=\"*20)\n",
    "    print(\"Fold: \", cnt)\n",
    "    print(\"=\"*20)\n",
    "    cnt +=1\n",
    "\n",
    "    x_train , x , y_train , y = train_test_split(cells[train_index] , labels[train_index] , \n",
    "                                                test_size = 0.2 ,\n",
    "                                                random_state = 11)\n",
    "\n",
    "    x_eval ,x_test ,y_eval , y_test = train_test_split(x , y , \n",
    "                                                        test_size = 0.5 , \n",
    "                                                        random_state = 11)\n",
    "\n",
    "    y_train = to_categorical(y_train, num_classes = 3)\n",
    "    y_eval = to_categorical(y_eval, num_classes = 3)\n",
    "    y_test = to_categorical(y_test, num_classes = 3)\n",
    "\n",
    "    # pca = PCA()\n",
    " \n",
    "    # x_train = pca.fit_transform(x_train)\n",
    "    # x_test = pca.transform(x_test)\n",
    "    # x_val = pca.transform(x_val)\n",
    "\n",
    "    #instantiate the model\n",
    "    height = 50\n",
    "    width = 27\n",
    "    classes = 3\n",
    "    channels = 4\n",
    "    epoch = 75\n",
    "    model = CNNbuild(height = height, width = width, classes = classes, channels = channels)\n",
    "    model.summary()\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy'])\n",
    "\n",
    "    #fit the model onto the dataset\n",
    "    h = model.fit(x_train, y_train, epochs = epoch, batch_size = 32,validation_data=(x_eval,y_eval),shuffle=True)\n",
    "\n",
    "    namamodel = 'Wavelet_model' + str(cnt) + '.pkl'\n",
    "    pickle.dump(model, open(namamodel, 'wb'))\n",
    "    #evaluate the model on test data\n",
    "    predictions = model.evaluate(x_test, y_test)\n",
    "    evaluation = model.evaluate(x_eval, y_eval)\n",
    "\n",
    "    print(f'LOSS : {evaluation[0]}')\n",
    "    print(f'ACCURACY : {evaluation[1]}')\n",
    "    print(f'LOSS : {predictions[0]}')\n",
    "    print(f'ACCURACY : {predictions[1]}')\n",
    "\n",
    "    predict_x=model.predict(x_test) \n",
    "    yhat_classes=np.argmax(predict_x,axis=1)\n",
    "    yhat_classes = np_utils.to_categorical(yhat_classes, num_classes = 3)\n",
    "\n",
    "    # print(\"Precision Score : \",precision_score(y_test, y_pred, \n",
    "    #                                            pos_label='positive'\n",
    "    #                                            average='micro'))\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    totalaccuracy.append(accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat_classes, average='macro')\n",
    "    print('Precision: %f' % precision)\n",
    "    totalprecision.append(precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat_classes, average='macro')\n",
    "    print('Recall: %f' % recall)\n",
    "    totalrecall.append(recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat_classes, average='macro')\n",
    "    print('F1 score: %f' % f1)\n",
    "    totalf1.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781931ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 20)\n",
    "print(\"rata-rata Akurasi: \",sum(totalaccuracy)/len(totalaccuracy))\n",
    "print(\"rata-rata Presisi: \",sum(totalprecision)/len(totalprecision))\n",
    "print(\"rata-rata Recall: \",sum(totalrecall)/len(totalrecall))\n",
    "print(\"rata-rata F1: \",sum(totalf1)/len(totalf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e18d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,8))\n",
    "plt.plot(range(epoch), h.history['loss'], label = 'Taining Loss')\n",
    "plt.plot(range(epoch), h.history['val_loss'], label = 'Validation Loss')\n",
    "plt.xlabel(\"Number of Epoch's\")\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Training Training & Validation Loss')\n",
    "plt.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9581cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,8))\n",
    "plt.plot(range(epoch), h.history['accuracy'], label = 'Taining Accuracy')\n",
    "plt.plot(range(epoch), h.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel(\"Number of Epoch's\")\n",
    "plt.ylabel('Accuracy Value')\n",
    "plt.title('Training Training & Validation Loss')\n",
    "plt.legend(loc = \"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d312f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
